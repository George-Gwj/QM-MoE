from __future__ import annotations

import re
from typing import Callable, Dict, List, Optional, Tuple

import torch
import numpy as np
from isaacgym import gymapi, gymtorch

from .env_add_baseinfo import IsaacGymEnv
from .state import EnvState, EnvSetup
from .control import Control
from .obs import EnvObservationAttribute, EnvSetupAttribute

import logging

from .utils import torch_rand_float

class IsaacGymEnvVelCmdArmSwing(IsaacGymEnv):
    def __init__(
        self,
        cfg,
        sim_params,
        sim_device,
        headless,
        controller,
        state_obs,
        setup_obs,
        privileged_state_obs,
        privileged_setup_obs,
        tasks,
        constraints,
        seed: int,
        dof_pos_reset_range_scale: float,
        obs_history_len: int,
        vis_resolution: Tuple[int, int],
        env_spacing: float,
        ctrl_buf_len: int,
        max_action_value: float,
        ctrl_delay: Optional[torch.Tensor] = None,
        init_dof_pos: Optional[torch.Tensor] = None,
        graphics_device_id: Optional[int] = None,
        debug_viz: float = True,
        attach_camera: bool = True,
        dense_rewards: bool = True,
    ):
        super().__init__(
            cfg=cfg,
            sim_params=sim_params,
            sim_device=sim_device,
            headless=headless,
            controller=controller,
            state_obs=state_obs,
            setup_obs=setup_obs,
            privileged_state_obs=privileged_state_obs,
            privileged_setup_obs=privileged_setup_obs,
            tasks=tasks,
            constraints=constraints,
            seed=seed,
            dof_pos_reset_range_scale=dof_pos_reset_range_scale,
            obs_history_len=obs_history_len,
            vis_resolution=vis_resolution,
            env_spacing=env_spacing,
            ctrl_buf_len=ctrl_buf_len,
            max_action_value=max_action_value,
            ctrl_delay=ctrl_delay,
            init_dof_pos=init_dof_pos,
            graphics_device_id=graphics_device_id,
            debug_viz=debug_viz,
            attach_camera=attach_camera,
            dense_rewards=dense_rewards,
        )

        # è¯¾ç¨‹å­¦ä¹ å‚æ•°
        # æ–°å¢ï¼šè¯¾ç¨‹å­¦ä¹ å¤šä¸€ä¸ªæœ€å‰é¢çš„é˜¶æ®µï¼Œcurriculum_stage=1æ—¶ä¸åšæœºæ¢°è‡‚ç›®æ ‡
        self.curriculum_stage = getattr(self.cfg.curriculum, 'stage', 2)  # å½“å‰é˜¶æ®µ
        self.curriculum_transition_steps = getattr(self.cfg.curriculum, 'transition_steps', 2000000)  # é˜¶æ®µè½¬æ¢æ­¥æ•°
        
        # å¥–åŠ±æƒé‡ï¼ˆå¯ä» cfg è¯»å–ï¼Œè‹¥ä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤ï¼‰
        self.base_cmd_tracking_sigma: float = getattr(
            self.cfg.rewards, "base_cmd_tracking_sigma", 0.5
        )
        self.base_cmd_reward_scale: float = getattr(
            self.cfg.rewards, "base_cmd_tracking_reward_scale", 3.0
        )
        
        # wzè·Ÿè¸ªå¥–åŠ±å‚æ•°
        self.wz_tracking_sigma: float = getattr(
            self.cfg.rewards, "wz_tracking_sigma", 0.3
        )
        self.wz_tracking_reward_scale: float = getattr(
            self.cfg.rewards, "wz_tracking_reward_scale", 1.5
        )
        
        # é«˜åº¦è·Ÿè¸ªå¥–åŠ±å‚æ•°
        self.height_tracking_sigma: float = getattr(
            self.cfg.rewards, "height_tracking_sigma", 0.05
        )
        self.height_tracking_reward_scale: float = getattr(
            self.cfg.rewards, "height_tracking_reward_scale", 2.0
        )
        
        # å¼ºåˆ¶å¿½ç•¥ taskï¼ˆä¾‹å¦‚æœ«ç«¯è·Ÿè¸ªï¼‰ï¼Œrewardç³»æ•°è®¾ä¸º0
        self.task_reward_attenuation: float = 0.0
        
        # ç›®æ ‡å…³èŠ‚è§’åº¦è·Ÿè¸ªå¥–åŠ±å‚æ•°
        self.arm_target_tracking_sigma: float = getattr(
            self.cfg.rewards, "arm_target_tracking_sigma", 0.1
        )
        self.arm_target_tracking_reward_scale: float = getattr(
            self.cfg.rewards, "arm_target_tracking_reward_scale", 2.0
        )
        
        # è¯¾ç¨‹å­¦ä¹ ä¸­çš„æœºæ¢°è‡‚å¥–åŠ±ç¼©æ”¾
        self.arm_reward_scale_by_stage = getattr(self.cfg.curriculum, 'arm_reward_scale_by_stage', True)

        # æœºæ¢°è‡‚å‚æ•° - ç›®æ ‡å…³èŠ‚è§’åº¦
        self.arm_dof_indices = self._find_arm_dof_indices()
        self.arm_target_pos: torch.Tensor = torch.zeros(
            (self.num_envs, len(self.arm_dof_indices)), dtype=torch.float32, device=self.device
        )
        
        # æœºæ¢°è‡‚ç›´æ¥æ§åˆ¶å‚æ•°
        self.arm_direct_control = getattr(self.cfg.curriculum, 'arm_direct_control', True)  # æ˜¯å¦ç›´æ¥æ§åˆ¶æœºæ¢°è‡‚
        
        # æœºæ¢°è‡‚éšæœºå§¿æ€å‚æ•°
        self.arm_random_pose_enabled = getattr(self.cfg.curriculum, 'arm_random_pose_enabled', True)
        self.arm_pose_range_scale = getattr(self.cfg.curriculum, 'arm_pose_range_scale', 0.5)  # æœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´
        
        # åŸºåº§é€Ÿåº¦æŒ‡ä»¤ç¼©æ”¾å‚æ•°
        self.base_vel_cmd_scale = getattr(self.cfg.curriculum, 'base_vel_cmd_scale', 0.0)  # åˆå§‹åŸºåº§é€Ÿåº¦ä¸º0
        
        # åŸºäºæ€§èƒ½çš„è¯¾ç¨‹å­¦ä¹ å‚æ•°
        self.performance_based_curriculum = getattr(self.cfg.curriculum, 'performance_based_curriculum', True)
        self.curriculum_eval_freq = getattr(self.cfg.curriculum, 'curriculum_eval_freq', 1000)  # æ¯1000æ­¥è¯„ä¼°ä¸€æ¬¡
        self.curriculum_eval_episodes = getattr(self.cfg.curriculum, 'curriculum_eval_episodes', 10)  # è¯„ä¼°10ä¸ªepisode
        
        # è¯¾ç¨‹å­¦ä¹ é˜ˆå€¼
        self.curriculum_thresholds = {
            'base_cmd_tracking_error': getattr(self.cfg.curriculum, 'base_cmd_tracking_error_threshold', 0.1),
            'arm_target_tracking_error': getattr(self.cfg.curriculum, 'arm_target_tracking_error_threshold', 0.05),
            'orientation_error': getattr(self.cfg.curriculum, 'orientation_error_threshold', 0.1),
        }
        
        # æ€§èƒ½å†å²è®°å½•
        self.performance_history = {
            'base_cmd_tracking_error': [],
            'arm_target_tracking_error': [],
            'orientation_error': [],
        }
        
        # è¯¾ç¨‹å­¦ä¹ çŠ¶æ€
        self.curriculum_stable_steps = 0  # å½“å‰é˜¶æ®µç¨³å®šæ­¥æ•°
        self.curriculum_min_stable_steps = getattr(self.cfg.curriculum, 'min_stable_steps', 2000)  # æœ€å°‘ç¨³å®šæ­¥æ•°
        
        # Episodeè®¡æ•°
        self.episode_count = 0  # æ€»episodeè®¡æ•°
        self.curriculum_stage_start_episode = 0  # å½“å‰é˜¶æ®µå¼€å§‹çš„episode
        self.curriculum_stage_start_step = 0  # å½“å‰é˜¶æ®µå¼€å§‹çš„æ­¥æ•°
        
        # é«˜åº¦æŒ‡ä»¤å‚æ•°
        self.height_range: Tuple[float, float] = getattr(
            self.cfg.rewards, "height_range", [0.15, 0.3]
        )
        
        # é«˜åº¦æŒ‡ä»¤å¼ é‡
        self.base_height_cmd: torch.Tensor = torch.zeros(
            (self.num_envs,), dtype=torch.float32, device=self.device
        )

    def _find_arm_dof_indices(self) -> List[int]:
        """
        æ‰¾åˆ°æ‰€æœ‰æœºæ¢°è‡‚å…³èŠ‚å¯¹åº”çš„dofç´¢å¼•
        å‡è®¾æœºæ¢°è‡‚å…³èŠ‚åç§°åŒ…å« 'link' æˆ– 'joint' ä¸”ä¸æ˜¯è…¿éƒ¨å…³èŠ‚
        """
        arm_indices = []
        if not hasattr(self, "dof_names"):
            return arm_indices
        
        # è…¿éƒ¨å…³èŠ‚åç§°æ¨¡å¼ï¼ˆé€šå¸¸ä¸åŒ…å«åœ¨æœºæ¢°è‡‚ä¸­ï¼‰
        leg_joint_patterns = ['hip', 'thigh', 'calf', 'foot', 'leg']
        
        for idx, name in enumerate(self.dof_names):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœºæ¢°è‡‚å…³èŠ‚ï¼ˆåŒ…å«linkæˆ–jointï¼Œä¸”ä¸æ˜¯è…¿éƒ¨å…³èŠ‚ï¼‰
            is_arm_joint = ('link' in name.lower() or 'joint' in name.lower()) and \
                          not any(pattern in name.lower() for pattern in leg_joint_patterns)
            
            if is_arm_joint:
                arm_indices.append(idx)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤çš„æœºæ¢°è‡‚å…³èŠ‚ç´¢å¼•ï¼ˆå‡è®¾å‰12ä¸ªæ˜¯è…¿éƒ¨ï¼Œåé¢æ˜¯æœºæ¢°è‡‚ï¼‰
        if not arm_indices and len(self.dof_names) > 12:
            # å‡è®¾ä»ç¬¬12ä¸ªå…³èŠ‚å¼€å§‹æ˜¯æœºæ¢°è‡‚
            arm_indices = list(range(12, min(len(self.dof_names), 18)))  # æœ€å¤š6ä¸ªæœºæ¢°è‡‚å…³èŠ‚
        
        return arm_indices

    def update_curriculum(self, global_step: int):
        """æ ¹æ®æ€§èƒ½æŒ‡æ ‡æ›´æ–°è¯¾ç¨‹é˜¶æ®µ - åŸºåº§é€Ÿåº¦æŒ‡ä»¤å’Œæœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦"""
        if not self.performance_based_curriculum:
            # ä½¿ç”¨åŸºäºæ­¥æ•°çš„ä¼ ç»Ÿè¯¾ç¨‹å­¦ä¹ 
            self._update_curriculum_by_steps(global_step)
            return
        
        # åŸºäºæ€§èƒ½çš„è¯¾ç¨‹å­¦ä¹ 
        if global_step % self.curriculum_eval_freq == 0 and global_step > 0:
            self._evaluate_performance()
            self._check_curriculum_transition()

    def _update_curriculum_by_steps(self, global_step: int):
        """ä¼ ç»Ÿçš„åŸºäºæ­¥æ•°çš„è¯¾ç¨‹å­¦ä¹ """
        # é˜¶æ®µ1ï¼šåªè®­ç»ƒå››è¶³ç«™ç«‹ï¼Œéœ€è¦åŸºåº§é€Ÿåº¦ç¨³å®šå’Œå§¿æ€ç¨³å®š
        if self.curriculum_stage == 1 and global_step >= self.curriculum_transition_steps:
            self.curriculum_stage = 2
            self.arm_pose_range_scale = 0.5  # æœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´
            self.base_vel_cmd_scale = 0.0    # åŸºåº§é€Ÿåº¦ä¸º0
            logging.info("è¯¾ç¨‹å­¦ä¹ ï¼šè¿›å…¥é˜¶æ®µ2 - åŸºåº§é€Ÿåº¦ä¸º0ï¼Œæœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´ï¼ˆå¼€å§‹æœºæ¢°è‡‚è®­ç»ƒï¼‰")
        
        # é˜¶æ®µ2ï¼šåŸºåº§é€Ÿåº¦å°èŒƒå›´ï¼Œæœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´
        elif self.curriculum_stage == 2 and global_step >= self.curriculum_transition_steps * 2:
            self.curriculum_stage = 3
            self.arm_pose_range_scale = 0.5  # æœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´
            self.base_vel_cmd_scale = 0.3    # åŸºåº§é€Ÿåº¦30%èŒƒå›´
            logging.info("è¯¾ç¨‹å­¦ä¹ ï¼šè¿›å…¥é˜¶æ®µ3 - åŸºåº§é€Ÿåº¦30%èŒƒå›´ï¼Œæœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´")
        
        # é˜¶æ®µ3ï¼šåŸºåº§é€Ÿåº¦ä¸­ç­‰èŒƒå›´ï¼Œæœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´
        elif self.curriculum_stage == 3 and global_step >= self.curriculum_transition_steps * 3:
            self.curriculum_stage = 4
            self.arm_pose_range_scale = 0.5  # æœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´
            self.base_vel_cmd_scale = 0.6    # åŸºåº§é€Ÿåº¦60%èŒƒå›´
            logging.info("è¯¾ç¨‹å­¦ä¹ ï¼šè¿›å…¥é˜¶æ®µ4 - åŸºåº§é€Ÿåº¦60%èŒƒå›´ï¼Œæœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´")
        
        # é˜¶æ®µ4ï¼šåŸºåº§é€Ÿåº¦å¤§èŒƒå›´ï¼Œæœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´
        elif self.curriculum_stage == 4 and global_step >= self.curriculum_transition_steps * 4:
            self.curriculum_stage = 5
            self.arm_pose_range_scale = 0.5  # æœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´
            self.base_vel_cmd_scale = 1.0    # åŸºåº§é€Ÿåº¦100%èŒƒå›´
            logging.info("è¯¾ç¨‹å­¦ä¹ ï¼šè¿›å…¥é˜¶æ®µ5 - åŸºåº§é€Ÿåº¦100%èŒƒå›´ï¼Œæœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´")

    def _evaluate_performance(self):
        """è¯„ä¼°å½“å‰æ€§èƒ½æŒ‡æ ‡"""
        # è®¡ç®—åŸºåº§é€Ÿåº¦è·Ÿè¸ªè¯¯å·®
        vel_error = torch.norm(self.state.local_root_lin_vel[:, :2] - self.base_lin_vel_cmd[:, :2], dim=1)
        base_cmd_error = torch.mean(vel_error).item()
        
        # è®¡ç®—æœºæ¢°è‡‚ç›®æ ‡è·Ÿè¸ªè¯¯å·® - å·²ç¦ç”¨
        arm_target_error = 0.0  # å›ºå®šä¸º0ï¼Œä¸è®¡ç®—æœºæ¢°è‡‚è¯¯å·®
        
        # è®¡ç®—å§¿æ€è¯¯å·®
        orientation_error = torch.mean(torch.norm(self.state.local_root_gravity[:, :2], dim=1)).item()
        
        # è®°å½•æ€§èƒ½å†å²
        self.performance_history['base_cmd_tracking_error'].append(base_cmd_error)
        self.performance_history['arm_target_tracking_error'].append(arm_target_error)
        self.performance_history['orientation_error'].append(orientation_error)
        
        # ä¿æŒå†å²è®°å½•é•¿åº¦
        max_history = 100
        for key in self.performance_history:
            if len(self.performance_history[key]) > max_history:
                self.performance_history[key] = self.performance_history[key][-max_history:]
        
        logging.info(f"æ€§èƒ½è¯„ä¼° - åŸºåº§é€Ÿåº¦è¯¯å·®: {base_cmd_error:.4f}, å§¿æ€è¯¯å·®: {orientation_error:.4f}, å½“å‰é€Ÿåº¦æŒ‡ä»¤: {self.base_lin_vel_cmd},å½“å‰é˜¶æ®µ: {self.curriculum_stage}")

    def _check_curriculum_transition(self):
        """æ£€æŸ¥æ˜¯å¦æ»¡è¶³è¿›å…¥ä¸‹ä¸€é˜¶æ®µçš„æ¡ä»¶"""
        if len(self.performance_history['base_cmd_tracking_error']) < 10:
            return  # éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
        
        # è®¡ç®—æœ€è¿‘10æ¬¡çš„å¹³å‡æ€§èƒ½
        recent_base_error = np.mean(self.performance_history['base_cmd_tracking_error'][-10:])
        recent_arm_error = np.mean(self.performance_history['arm_target_tracking_error'][-10:])
        recent_orientation_error = np.mean(self.performance_history['orientation_error'][-10:])
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³å½“å‰é˜¶æ®µçš„æ€§èƒ½è¦æ±‚
        stage_requirements_met = self._check_stage_requirements(
            recent_base_error, recent_arm_error, recent_orientation_error
        )
        
        if stage_requirements_met:
            self.curriculum_stable_steps += self.curriculum_eval_freq
        else:
            self.curriculum_stable_steps = 0  # é‡ç½®ç¨³å®šæ­¥æ•°
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
        if (stage_requirements_met and 
            self.curriculum_stable_steps >= self.curriculum_min_stable_steps and
            self.curriculum_stage < 5):
            self._advance_curriculum_stage()

    def _check_stage_requirements(self, base_error, arm_error, orientation_error):
        """æ£€æŸ¥å½“å‰é˜¶æ®µæ˜¯å¦æ»¡è¶³æ€§èƒ½è¦æ±‚"""
        if self.curriculum_stage == 1:
            # é˜¶æ®µ1ï¼šåªè®­ç»ƒå››è¶³ç«™ç«‹ï¼Œéœ€è¦åŸºåº§é€Ÿåº¦ç¨³å®šå’Œå§¿æ€ç¨³å®š
            return (base_error < self.curriculum_thresholds['base_cmd_tracking_error'] and
                    orientation_error < self.curriculum_thresholds['orientation_error'])
        
        elif self.curriculum_stage == 2:
            # é˜¶æ®µ2ï¼šåŸºåº§é€Ÿåº¦ä¸º0ï¼Œæœºæ¢°è‡‚ç›®æ ‡è·Ÿè¸ªå’Œå§¿æ€ç¨³å®š - å·²ç¦ç”¨æœºæ¢°è‡‚è¯¯å·®æ£€æŸ¥
            return (base_error < self.curriculum_thresholds['base_cmd_tracking_error'] and
                    orientation_error < self.curriculum_thresholds['orientation_error'])
        
        elif self.curriculum_stage == 3:
            # é˜¶æ®µ3ï¼šåŸºåº§é€Ÿåº¦30%èŒƒå›´ï¼Œéœ€è¦åŸºåº§é€Ÿåº¦è·Ÿè¸ª - å·²ç¦ç”¨æœºæ¢°è‡‚è¯¯å·®æ£€æŸ¥
            return (base_error < self.curriculum_thresholds['base_cmd_tracking_error'] and
                    orientation_error < self.curriculum_thresholds['orientation_error'])
        
        elif self.curriculum_stage == 4:
            # é˜¶æ®µ4ï¼šåŸºåº§é€Ÿåº¦60%èŒƒå›´ï¼Œéœ€è¦åŸºåº§é€Ÿåº¦è·Ÿè¸ª - å·²ç¦ç”¨æœºæ¢°è‡‚è¯¯å·®æ£€æŸ¥
            return (base_error < self.curriculum_thresholds['base_cmd_tracking_error'] and
                    orientation_error < self.curriculum_thresholds['orientation_error'])
        
        return False

    def _advance_curriculum_stage(self):
        """è¿›å…¥ä¸‹ä¸€ä¸ªè¯¾ç¨‹é˜¶æ®µ"""
        old_stage = self.curriculum_stage
        self.curriculum_stage += 1
        self.curriculum_stable_steps = 0  # é‡ç½®ç¨³å®šæ­¥æ•°
        
        # è®¡ç®—å½“å‰é˜¶æ®µç»è¿‡çš„episodeå’Œæ­¥æ•°
        episodes_in_stage = self.episode_count - self.curriculum_stage_start_episode
        steps_in_stage = self.global_step - self.curriculum_stage_start_step
        
        # æ›´æ–°è¯¾ç¨‹å‚æ•°
        if self.curriculum_stage == 1:
            self.arm_pose_range_scale = 0.5
            self.base_vel_cmd_scale = 0.0
            stage_description = "åŸºåº§é€Ÿåº¦ä¸º0ï¼Œåªè®­ç»ƒå››è¶³ç«™ç«‹ï¼ˆåŸºåº§é€Ÿåº¦ç¨³å®š+å§¿æ€ç¨³å®šï¼‰"
        
        elif self.curriculum_stage == 2:
            self.arm_pose_range_scale = 0.5
            self.base_vel_cmd_scale = 0.0
            stage_description = "åŸºåº§é€Ÿåº¦ä¸º0ï¼Œæœºæ¢°è‡‚ç›´æ¥ä¼ é€’é‡‡æ ·å…³èŠ‚è§’åº¦ï¼ˆç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´ï¼‰"
        
        elif self.curriculum_stage == 3:
            self.arm_pose_range_scale = 0.5
            self.base_vel_cmd_scale = 0.3
            stage_description = "åŸºåº§é€Ÿåº¦30%èŒƒå›´ï¼Œæœºæ¢°è‡‚ç›´æ¥ä¼ é€’é‡‡æ ·å…³èŠ‚è§’åº¦ï¼ˆç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´ï¼‰"
        
        elif self.curriculum_stage == 4:
            self.arm_pose_range_scale = 0.5
            self.base_vel_cmd_scale = 0.6
            stage_description = "åŸºåº§é€Ÿåº¦60%èŒƒå›´ï¼Œæœºæ¢°è‡‚ç›´æ¥ä¼ é€’é‡‡æ ·å…³èŠ‚è§’åº¦ï¼ˆç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´ï¼‰"
        
        elif self.curriculum_stage == 5:
            self.arm_pose_range_scale = 0.5
            self.base_vel_cmd_scale = 1.0
            stage_description = "åŸºåº§é€Ÿåº¦100%èŒƒå›´ï¼Œæœºæ¢°è‡‚ç›´æ¥ä¼ é€’é‡‡æ ·å…³èŠ‚è§’åº¦ï¼ˆç›®æ ‡å…³èŠ‚è§’åº¦50%èŒƒå›´ï¼‰"
        
        # æ›´æ–°é˜¶æ®µå¼€å§‹è®¡æ•°
        self.curriculum_stage_start_episode = self.episode_count
        self.curriculum_stage_start_step = self.global_step
        
        # åªåœ¨è¯¾ç¨‹è½¬æ¢æ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯
        print("=" * 80)
        print(f"ğŸ“ è¯¾ç¨‹å­¦ä¹ è½¬æ¢ - ä»é˜¶æ®µ{old_stage}è¿›å…¥é˜¶æ®µ{self.curriculum_stage}")
        print(f"ğŸ“Š é˜¶æ®µ{old_stage}ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - ç»è¿‡çš„episodeæ•°: {episodes_in_stage}")
        print(f"   - ç»è¿‡çš„æ­¥æ•°: {steps_in_stage}")
        print(f"   - æ€»episodeæ•°: {self.episode_count}")
        print(f"   - æ€»æ­¥æ•°: {self.global_step}")
        print(f"ğŸ¯ é˜¶æ®µ{self.curriculum_stage}ç›®æ ‡: {stage_description}")
        print(f"âš™ï¸  è¯¾ç¨‹å‚æ•°:")
        print(f"   - æœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦èŒƒå›´: {self.arm_pose_range_scale * 100}%")
        print(f"   - åŸºåº§é€Ÿåº¦æŒ‡ä»¤ç¼©æ”¾: {self.base_vel_cmd_scale * 100}%")
        print("=" * 80)
        
        # åŒæ—¶è®°å½•åˆ°æ—¥å¿—
        logging.info(f"è¯¾ç¨‹å­¦ä¹ ï¼šä»é˜¶æ®µ{old_stage}è¿›å…¥é˜¶æ®µ{self.curriculum_stage}")
        logging.info(f"é˜¶æ®µ{old_stage}ç»Ÿè®¡ - Episodes: {episodes_in_stage}, Steps: {steps_in_stage}")
        logging.info(f"é˜¶æ®µ{self.curriculum_stage}ç›®æ ‡: {stage_description}")

    def _generate_target_arm_poses(self, env_ids: torch.Tensor) -> torch.Tensor:
        """ä¸ºæŒ‡å®šç¯å¢ƒç”Ÿæˆç›®æ ‡æœºæ¢°è‡‚å…³èŠ‚è§’åº¦ï¼ŒåŸºäºè¯¾ç¨‹å­¦ä¹ """
        if not self.arm_dof_indices or not self.arm_random_pose_enabled:
            return torch.zeros((len(env_ids), len(self.arm_dof_indices)), device=self.device)
        
        num_envs = len(env_ids)
        random_poses = torch.zeros((num_envs, len(self.arm_dof_indices)), device=self.device)
        
        # æ£€æŸ¥å…³èŠ‚é™ä½æ˜¯å¦å·²ç»åˆå§‹åŒ–
        if not hasattr(self, 'curr_dof_pos_limits') or self.curr_dof_pos_limits is None:
            print("è­¦å‘Š: å…³èŠ‚é™ä½æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤éšæœºå§¿æ€å‚æ•°")
            # ä½¿ç”¨é»˜è®¤èŒƒå›´
            for i, dof_idx in enumerate(self.arm_dof_indices):
                random_poses[:, i] = torch_rand_float(
                    lower=-1.0, upper=1.0, shape=(num_envs,), 
                    device=self.device, generator=self.generator
                )
        else:
            # åŸºäºåˆå§‹ä½ç½®é™„è¿‘éšæœºåŒ–ï¼ˆç±»ä¼¼çˆ¶ç±»é€»è¾‘ï¼‰
            for i, dof_idx in enumerate(self.arm_dof_indices):
                joint_limit_low = self.curr_dof_pos_limits[dof_idx, 0]
                joint_limit_high = self.curr_dof_pos_limits[dof_idx, 1]
                joint_range = joint_limit_high - joint_limit_low
                
                # è·å–åˆå§‹ä½ç½®
                init_pos = self.init_dof_pos[0, dof_idx]  # å‡è®¾æ‰€æœ‰ç¯å¢ƒçš„åˆå§‹ä½ç½®ç›¸åŒ
                
                # æ ¹æ®è¯¾ç¨‹é˜¶æ®µè°ƒæ•´éšæœºèŒƒå›´
                effective_range = joint_range * self.arm_pose_range_scale
                
                # åœ¨åˆå§‹ä½ç½®é™„è¿‘ç”Ÿæˆéšæœºå™ªå£°ï¼ˆç±»ä¼¼çˆ¶ç±»é€»è¾‘ï¼‰
                noise = (
                    self.dof_pos_reset_range_scale
                    * torch.randn(
                        num_envs,
                        device=self.device,
                        generator=self.generator,
                    )
                    * effective_range
                )
                
                # åŸºäºåˆå§‹ä½ç½® + å™ªå£°ç”Ÿæˆéšæœºè§’åº¦
                random_poses[:, i] = init_pos + noise
                
                # ç¡®ä¿ä¸è¶…å‡ºå…³èŠ‚é™ä½
                random_poses[:, i] = torch.clamp(
                    random_poses[:, i], joint_limit_low, joint_limit_high
                )
        
        return random_poses


    # è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
    def _reward_base_cmd_tracking(self, state: EnvState, control: Control):
        """åŸºåº§é€Ÿåº¦æŒ‡ä»¤è·Ÿè¸ªå¥–åŠ±"""
        vel_error = state.local_root_lin_vel[:, :2] - self.base_lin_vel_cmd[:, :2]
        return torch.exp(-torch.sum(vel_error**2, dim=1) / self.base_cmd_tracking_sigma**2)

    def _reward_wz_tracking(self, state: EnvState, control: Control):
        """wzè§’é€Ÿåº¦æŒ‡ä»¤è·Ÿè¸ªå¥–åŠ±"""
        wz_error = state.local_root_ang_vel[:, 2] - self.base_ang_vel_cmd[:, 2]
        return torch.exp(-torch.square(wz_error) / self.wz_tracking_sigma**2)

    def _reward_height_tracking(self, state: EnvState, control: Control):
        """é«˜åº¦æŒ‡ä»¤è·Ÿè¸ªå¥–åŠ±"""
        height_error = state.root_pos[:, 2] - self.base_height_cmd
        return torch.exp(-torch.square(height_error) / self.height_tracking_sigma**2)

    def _reward_ang_vel_xy_penalty(self, state: EnvState, control: Control):
        """rollå’Œpitchè§’é€Ÿåº¦æƒ©ç½š"""
        return torch.sum(torch.square(state.local_root_ang_vel[:, :2]), dim=1)

    def _reward_orientation_penalty(self, state: EnvState, control: Control):
        """å§¿æ€æƒ©ç½šï¼ˆéæ°´å¹³åŸºåº§ï¼‰"""
        return torch.sum(torch.square(state.local_root_gravity[:, :2]), dim=1)

    def _reward_arm_target_tracking(self, state: EnvState, control: Control):
        """æœºæ¢°è‡‚ç›®æ ‡å…³èŠ‚è§’åº¦è·Ÿè¸ªå¥–åŠ± - å·²ç¦ç”¨ï¼Œæœºæ¢°è‡‚ç›´æ¥ä¼ é€’é‡‡æ ·å…³èŠ‚è§’åº¦"""
        # æœºæ¢°è‡‚ç°åœ¨ç›´æ¥ä¼ é€’é‡‡æ ·çš„å…³èŠ‚è§’åº¦ï¼Œä¸éœ€è¦policyå­¦ä¹ æœºæ¢°è‡‚æ§åˆ¶
        # å› æ­¤æœºæ¢°è‡‚å¥–åŠ±å§‹ç»ˆä¸º0
        return torch.zeros(self.num_envs, device=self.device, dtype=torch.float)

    def compute_reward(self, state: EnvState, control: Control):
        """é‡å†™å¥–åŠ±è®¡ç®—ï¼Œéµå¾ªçˆ¶ç±»æ¡†æ¶"""
        return_dict = {
            "total": torch.zeros(self.num_envs, device=self.device, dtype=torch.float),
            "env": torch.zeros(self.num_envs, device=self.device, dtype=torch.float),
            "constraint": torch.zeros(self.num_envs, device=self.device, dtype=torch.float),
            "task": torch.zeros(self.num_envs, device=self.device, dtype=torch.float),
        }
        
        # 1. è®¡ç®—ç¯å¢ƒå¥–åŠ±ï¼ˆåŸºç¡€å¥–åŠ±ï¼‰- ä½¿ç”¨çˆ¶ç±»æ¡†æ¶
        for i in range(len(self.reward_functions)):
            name = self.reward_names[i]
            return_dict[name] = (
                self.reward_functions[i](state=state, control=control)
                * self.reward_scales[name]
            )
            return_dict["total"] += return_dict[name]
            return_dict["env"] += return_dict[name]
        
        # 2. è®¡ç®—çº¦æŸå¥–åŠ±ï¼ˆä¿æŒåŸæœ‰ï¼‰
        for constraint_name, constraint in self.constraints.items():
            constraint_rewards = {
                f"constraint/{constraint_name}/{k}": v
                for k, v in constraint.reward(state=state, control=control).items()
            }
            return_dict.update(constraint_rewards)
            return_dict["total"] += sum(constraint_rewards.values())
            return_dict["constraint"] += sum(constraint_rewards.values())
        
        # # 3. è®¡ç®—ä»»åŠ¡å¥–åŠ±ï¼ˆä¿æŒåŸæœ‰ï¼‰
        # for task_name, task in self.tasks.items():
        #     raw_task_rewards = task.reward(state=state, control=control)
        #     attenuated = {k: v * self.task_reward_attenuation for k, v in raw_task_rewards.items()}
        #     task_rewards = {
        #         f"task/{task_name}/{k}": v
        #         for k, v in attenuated.items()
        #     }
        #     return_dict.update(task_rewards)
        #     return_dict["total"] += sum(task_rewards.values())
        #     return_dict["task"] += sum(task_rewards.values())
        
        # 4. åå¤„ç†ï¼ˆéµå¾ªçˆ¶ç±»é€»è¾‘ï¼‰
        if self.cfg.rewards.only_positive_rewards:
            return_dict["total"][:] = torch.clip(return_dict["total"][:], min=0.0)
        return_dict["task_to_env_ratio"] = return_dict["task"].abs() / (
            return_dict["env"].abs() + 1e-10
        )
        return_dict["task_to_constraint_ratio"] = return_dict["task"].abs() / (
            return_dict["constraint"].abs() + 1e-10
        )
        
        # 5. è¿”å›ï¼ˆéµå¾ªçˆ¶ç±»æ ¼å¼ï¼‰
        return {f"reward/{k}": v * self.reward_dt_scale for k, v in return_dict.items()}

    def get_observations(
        self,
        state: EnvState,
        setup: EnvSetup,
        state_obs: Dict[str, EnvObservationAttribute],
        setup_obs: Dict[str, EnvSetupAttribute],
    ):
        """é‡å†™è§‚æµ‹æ–¹æ³•ï¼Œæ·»åŠ é«˜åº¦æŒ‡ä»¤åˆ°è§‚æµ‹ä¸­"""
        obs_attrs = []
        for name, obs_attr in state_obs.items():
            value = obs_attr(struct=state, generator=self.generator)
            assert value.shape[-1] == obs_attr.dim
            obs_attrs.append(value)
        state_obs_tensor = torch.cat(
            obs_attrs,
            dim=1,
        )

        if len(self.tasks) > 0:
            all_task_obs = []
            for k, task in self.tasks.items():
                task_obs = task.observe(state=state)
                all_task_obs.append(task_obs)
            task_obs_tensor = torch.cat(
                all_task_obs,
                dim=1,
            )
        else:
            task_obs_tensor = torch.zeros(
                (self.num_envs, 0), dtype=torch.float, device=self.device
            )
        if len(setup_obs) > 0:
            obs_attrs = []
            for k, obs_attr in setup_obs.items():
                value = obs_attr(struct=setup, generator=self.generator).reshape(
                    self.num_envs, -1
                )
                assert value.shape[-1] == obs_attr.dim
                obs_attrs.append(value)
            setup_obs_tensor = torch.cat(obs_attrs, dim=1)
        else:
            setup_obs_tensor = torch.zeros(
                (self.num_envs, 0), dtype=torch.float, device=self.device
            )

        # æ·»åŠ æ–°çš„è‡ªå®šä¹‰è§‚æµ‹
        additional_obs_tensor = self._get_additional_observations(state)

        return torch.cat(
            (
                setup_obs_tensor,
                state_obs_tensor,
                task_obs_tensor,
                # è¿½åŠ  baselink é€Ÿåº¦æŒ‡ä»¤ï¼ˆvx, vy, vzï¼‰ï¼Œä»…ä½œä¸ºè§‚æµ‹
                self.base_lin_vel_cmd,
                # è¿½åŠ  baselink è§’é€Ÿåº¦æŒ‡ä»¤ï¼ˆwx, wy, wzï¼‰ï¼Œä»…ä½œä¸ºè§‚æµ‹
                self.base_ang_vel_cmd,
                # è¿½åŠ  baselink é«˜åº¦æŒ‡ä»¤ï¼ˆzï¼‰ï¼Œä»…ä½œä¸ºè§‚æµ‹
                self.base_height_cmd.unsqueeze(-1),
                additional_obs_tensor,  # æ–°å¢çš„è§‚æµ‹
                self.ctrl.action,
            ),
            dim=1,
        )

    def reset_idx(self, env_ids):
        """é‡å†™ reset_idx æ–¹æ³•ï¼Œæ·»åŠ è¯¾ç¨‹å­¦ä¹ çš„åŸºåº§é€Ÿåº¦æŒ‡ä»¤å’Œç›®æ ‡å…³èŠ‚è§’åº¦ç”Ÿæˆ"""
        if len(env_ids) == 0:
            return

        # 1. ä½¿ç”¨çˆ¶ç±»çš„æ ‡å‡†é‡ç½®é€»è¾‘
        super().reset_idx(env_ids)
        
        # 2. æ ¹æ®è¯¾ç¨‹å­¦ä¹ é‡æ–°è®¾ç½®åŸºåº§é€Ÿåº¦æŒ‡ä»¤
        if len(env_ids) > 0:
            num = len(env_ids)
            # åŸºåº§çº¿é€Ÿåº¦æŒ‡ä»¤ï¼ˆæ ¹æ®è¯¾ç¨‹å­¦ä¹ ç¼©æ”¾ï¼‰
            vx_vy = torch_rand_float(
                lower=-1.0,
                upper=1.0,
                shape=(num, 2),
                device=self.device,
                generator=self.generator,
            ) * self.base_vel_cmd_scale
            self.base_lin_vel_cmd[env_ids, 0:2] = vx_vy
            self.base_lin_vel_cmd[env_ids, 2] = 0.0
            
            # åŸºåº§è§’é€Ÿåº¦æŒ‡ä»¤ï¼ˆæ ¹æ®è¯¾ç¨‹å­¦ä¹ ç¼©æ”¾ï¼‰
            wz = torch_rand_float(
                lower=-0.5,
                upper=0.5,
                shape=(num, 1),
                device=self.device,
                generator=self.generator,
            ) * self.base_vel_cmd_scale
            wx_wy_wz = torch.zeros((num, 3), device=self.device)
            wx_wy_wz[:, 2] = wz[:, 0]
            self.base_ang_vel_cmd[env_ids] = wx_wy_wz
        
        # 3. ç”Ÿæˆç›®æ ‡æœºæ¢°è‡‚å…³èŠ‚è§’åº¦ï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰
        if len(env_ids) > 0 and self.arm_dof_indices and self.arm_random_pose_enabled:
            target_arm_poses = self._generate_target_arm_poses(env_ids)
            self.arm_target_pos[env_ids] = target_arm_poses
        
        # 4. è®¾ç½®é«˜åº¦æŒ‡ä»¤ï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰
        if len(env_ids) > 0:
            height_cmd = torch_rand_float(
                lower=self.height_range[0],
                upper=self.height_range[1],
                shape=(len(env_ids),),
                device=self.device,
                generator=self.generator,
            )
            self.base_height_cmd[env_ids] = height_cmd

            
    def step(
        self,
        action: torch.Tensor,
        return_vis: bool = False,
        callback: Optional[Callable[[IsaacGymEnv], None]] = None,
    ):
        # æ›´æ–°è¯¾ç¨‹é˜¶æ®µ
        self.update_curriculum(self.global_step)
        
        # å¦‚æœå¯ç”¨æœºæ¢°è‡‚ç›´æ¥æ§åˆ¶ï¼Œéœ€è¦ä¿®æ”¹action
        if self.arm_direct_control and self.arm_dof_indices:
            # å°†æœºæ¢°è‡‚é‡‡æ ·çš„ç›®æ ‡å…³èŠ‚è§’åº¦ç›´æ¥ä¼ é€’ç»™action
            # å‡è®¾actionçš„å‰é¢éƒ¨åˆ†æ˜¯è…¿éƒ¨æ§åˆ¶ï¼Œåé¢éƒ¨åˆ†æ˜¯æœºæ¢°è‡‚æ§åˆ¶
            if action.shape[1] >= len(self.arm_dof_indices):
                # å¦‚æœactionåŒ…å«æœºæ¢°è‡‚éƒ¨åˆ†ï¼Œæ›¿æ¢ä¸ºé‡‡æ ·çš„ç›®æ ‡å…³èŠ‚è§’åº¦
                action[:, -len(self.arm_dof_indices):] = self.arm_target_pos
            else:
                # å¦‚æœactionä¸åŒ…å«æœºæ¢°è‡‚éƒ¨åˆ†ï¼Œéœ€è¦æ‰©å±•action
                leg_action = action
                full_action = torch.cat([leg_action, self.arm_target_pos], dim=1)
                action = full_action
        
        # è°ƒç”¨çˆ¶ç±»çš„stepæ–¹æ³•
        return super().step(action=action, return_vis=return_vis, callback=callback)